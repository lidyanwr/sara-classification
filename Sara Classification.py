# -*- coding: utf-8 -*-
"""Assignment Sara Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvYNoYq_VTQtDWeBRaX71fXPf795zFfL
"""

#%%sh
#!pip install pandas
# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

import os
os.listdir(os.path.join('gdrive', 'My Drive', 'BABE AI', 'LogisticRegression', 'data', 'dicts'))

data_dir = os.path.join('gdrive', 'My Drive', 'BABE AI', 'LogisticRegression', 'data')
# Reading training texts
cmt_norm_file = os.path.join(data_dir, 'sara', 'normal_comments.txt')
cmt_sara_file = os.path.join(data_dir, 'sara', 'sara_comments.txt')
#cmt_norm_test_file = os.path.join(data_dir, 'sara', 'test_data', 'normal_comments.txt')
#cmt_sara_test_file = os.path.join(data_dir, 'sara', 'test_data', 'sara_comments.txt')

# Read texts into list
def read_lines(filepath):
    with open(filepath) as fp:
        content = fp.readlines()
        content = [x.strip() for x in content]
    return content

cmt_norm = read_lines(cmt_norm_file)
cmt_sara = read_lines(cmt_sara_file)
#cmt_norm_test = read_lines(cmt_norm_test_file)
#cmt_sara_test = read_lines(cmt_sara_test_file)
print('data set size: ',len(cmt_norm), len(cmt_sara))

#cmt_norm_all = cmt_norm_train + cmt_norm_test
#cmt_sara_all = cmt_sara_train + cmt_sara_test

#print(len(cmt_norm_all), len(cmt_sara_all))

import pandas as pd
# create a dataframe for all training texts, with their labels
def create_dataframe_with_label(cmt_norm, cmt_sara):
  cmt_all = cmt_norm + cmt_sara
  # make label
  label = []
  for _ in cmt_norm:
    label.append(0)
  for _ in cmt_sara:
    label.append(1)
  # create a pandas dataframe using texts and labels
  trainDF = pd.DataFrame()
  trainDF['text'] = cmt_all
  trainDF['label'] = label
  return trainDF

data = create_dataframe_with_label(cmt_norm, cmt_sara)
print(data.head(20))

print(data.sample(30))

def load_stop_words():
  # Get the set of stopwords
  stop_words_f = os.path.join(data_dir, 'dicts', 'stop_words.txt')

  flines = read_lines(stop_words_f)
  return set([x.strip() for x in flines])

stop_words = load_stop_words()
print(len(stop_words))

#install beautifulSoup
#!pip install bs4
# For cleansing
from bs4 import BeautifulSoup
import re
# text cleansing function
def raw_to_words(raw_text, stop_words=None):
    # 1. Remove HTML
    text_1 = BeautifulSoup(raw_text).get_text()
    
    # 2. Remove non-letters with regex
    letters_only = re.sub("[^a-zA-Z]", " ", text_1)
    
    # 3. Convert to lower case, split into individual words
    words = letters_only.lower().split()
    
    # 4. Remove stop words
    if stop_words:
        meaningful_words = [w for w in words if not w in stop_words]   
    else:
        meaningful_words = words
    
    # 5. Join the words back into one string separated by space & return
    return(" ".join(meaningful_words))

# check to see how the cleansing function works
print(cmt_norm[1])
print(raw_to_words(cmt_norm[1], stop_words=stop_words))
print(raw_to_words(cmt_norm[1], stop_words=stop_words))

clean_data = data
clean_data['text'] = clean_data['text'].apply(raw_to_words, stop_words=stop_words)
clean_data = clean_data.loc[clean_data['text']!='']
print(clean_data.head())

# Commented out IPython magic to ensure Python compatibility.
# Plot
#import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

def show_wordcloud(data, stop_words, title=None):
    wordcloud = WordCloud(
        background_color='white',
        stopwords=stop_words,
        #max_words=200,
        max_font_size=80,
        width=800,
        height=600,
        ).generate(str(data))
    fig = plt.figure(1, figsize=(8, 6))
    plt.axis('off')
    if title:
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.show()

#get negative samples
neg_samples = clean_data.loc[clean_data['label'] == 0]

# get positive sampels
pos_samples = clean_data.loc[clean_data['label'] == 1]

bigtext_sara = " ".join(list(pos_samples['text']))
print(len(bigtext_sara))
bigtext_norm = " ".join(list(neg_samples['text']))
print(len(bigtext_norm))

show_wordcloud(bigtext_sara, stop_words)

show_wordcloud(bigtext_norm, stop_words)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from sklearn import decomposition, ensemble
from sklearn.model_selection import cross_val_score

# split the dataset into training and validation datasets
train_x, valid_x, train_y, valid_y = model_selection.train_test_split(clean_data['text'], clean_data['label'])
print(len(train_x), len(valid_x))

# create a count vectorizer
count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
count_vect.fit(clean_data['text'])
# transform the training and validation data using count vectorizer
xtrain_count = count_vect.transform(train_x)
xvalid_count = count_vect.transform(valid_x)
print(xtrain_count.shape, xvalid_count.shape)

# word level tf-idf
tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}')
tfidf_vect.fit(clean_data['text'])
xtrain_tfidf = tfidf_vect.transform(train_x)
xvalid_tfidf = tfidf_vect.transform(valid_x)
print(xtrain_tfidf.shape, xvalid_tfidf.shape)

#train model
logistic_r = linear_model.LogisticRegression()
logistic_r.fit(xtrain_tfidf, train_y)

# get the predictions
pred_valid = logistic_r.predict(xvalid_tfidf)
print("Validation set")
print("Accuracy:", metrics.accuracy_score(pred_valid, valid_y))
print("Report:\n", metrics.classification_report(valid_y, pred_valid))